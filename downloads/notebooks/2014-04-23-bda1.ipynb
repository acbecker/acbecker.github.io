{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Bayesian Data Analysis : Intro"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Terminology and Notation:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$p(\\theta | y)$ : Probability of model parameters $\\theta$ given data $y$.  Probability of model parameters \"conditional\" on the data.  Also known as the _posterior_ distribution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$p(y|\\theta)$ : Probabilty of data, given the model with parameters $\\theta$.  Also known as the _sampling_ or _data_ distribution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$p(\\theta)$ : _Prior_ distribution of the model parameters.  Typically specified by the modeler (i.e. a degree of freedom in your analysis)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bayes Rule : $p(\\theta|y) \\propto  p(\\theta) \\cdot p(y|\\theta)$\n",
      " $$ $$\n",
      "Or : $ln(posterior) \\propto  ln(prior) + ln(data\\ likelihood)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Frequentists work with $p(y|\\theta)$.  That is, they typically specify a model and look at the $\\chi^2$ value of the data.  Different models may be compared based on their $\\chi^2$ per degree of freedom, where the degrees of freedom are the number of data points fitted, minus the number of terms fitted.  As you make a model more and more complex, the $\\chi^2$ will _always_ go down.  But as you start to **overfit** the problem, the $\\Delta \\chi^2$ will become smaller and smaller, indicating less and less relevance for the new model parameters.  The predictive power of this model will also decrease, indicating it becomes a high _variance_ estimator.  You always want to trade off **bias** and **variance** when fitting models to data.\n",
      "\n",
      "For a normal distribution the probability density function is: $f(x, \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 * \\pi}} e^\\frac{(x - \\mu)^2}{2 \\sigma^2}$.  Looking at e.g. fitting data to a line (y = m x + b), we have $\\mu$ determined by the model ($\\mu = mx + b$) and $\\sigma$ the uncertainties on your data.  Looking at this in a Bayesian context, we have $ln\\ p(y|x,n,m,b,\\sigma) = -0.5 \\sum_n$$ \\frac{(y_n - (m * x_n + b))^2}{\\sigma_n^2} + ln(2\\pi\\sigma_n^2)$.  Since $\\sigma_n$ are typically fixed, you can approximate $ln(data\\ likelihood) = -0.5 * \\chi^2$.  This indicates a close connection between the \"standard\" way of looking at a data vs. model (minimize $\\chi^2$) vs the Bayesian way of doing things (maximize posterior likelihood).  The main difference is in $p(\\theta)$, or the prior distribution of your parameters.  This is the a bone of contention in Bayesian analysis, but it enables a whole new type of data modeling if used appropriately.  It also allows us to use our understanding about the world to impact our analysis, for good or for evil."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Comparing Frequentist and Bayesian Approaches:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The frequentist way of looking at things is to think of probabilities in terms of repeated measurements.  You make a series of repeated measurements, and uncover the probability distribution through the number of times a given value comes up, or by the moments of all the numbers (mean, standard deviation).  Underlying this is the assumption that there is a single true value you are trying to uncover, and the noise comes from your system.  Another way of saying it: probabilities and frequencies are directly related.  A downside of this view is that science infrequently proceeds by making repeated measurements of the exact same effect in the exact same way.  \n",
      "\n",
      " * One way of reducing this is to say its hypothesis testing: compare to the null hypothesis; what is the p-value or significance of your results.\n",
      " \n",
      " * In terms of simulation, you can draw data from the model: $p(y|\\theta)$.  \n",
      " \n",
      " * The studies are repeatable, the underlying parameters are fixed.  \n",
      "  \n",
      " * In repeated sampling, you have 68% of the intervals covering the true parameter.\n",
      "\n",
      "The Bayesian way of looking at things is to think of probabilities in terms of confidence limits.  Given your observation of the data, your confidence in the model is such and such.  In this case probabilities are related to our own knowledge of an event, and yield probaility distributions of parameter values.  Which to me seems more natural, but is a bit more difficult to represent in practice (not a single number with error bars, but a full posterior distribution).  \n",
      "\n",
      " * The reductionist way of thinking about this is that its interval estimation.  \n",
      " \n",
      " * In terms of simulation, you can draw models from the data: $p(\\theta | y)$.  \n",
      " \n",
      " * The data are fixed.  \n",
      " \n",
      " * With probability 68%, the parameter is in the interval.  "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}