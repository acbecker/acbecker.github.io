Title: Odds of Correctly Predicting the NCAA Basketball Tournament
date: 2014-02-10 10:58
comments: true
slug: ncaa
----

<!-- PELICAN_BEGIN_SUMMARY --> 

I was briefly interviewed for a 
[Wall Street Journal](http://online.wsj.com/news/articles/SB10001424052702304450904579367153999135482?mod=Business_newsreel_3)
article on Warren Buffett's insuring of a $\$1$ billion dollar prize for
any contestant who correctly picks all 63 games in the 2013 NCAA
basketball tournament (see also
[here](http://www.marketwatch.com/story/yahoo-warren-buffett-and-a-1-billion-contest-2014-02-09-194491044)).  Don't ask
how this happened; it involves consorting with a bunch of degenerates.  As stated in the 
article, "Mr. Buffett's Berkshire Hathaway would take on the risk, and earn a fee for doing so".  My position
as stated in the article is that this paid premium to Berkshire Hathaway was unnecessary, since the odds of picking the outcome of these 63 games
correctly is so unlikely.  A great deal for Mr. Buffett, but that is indeed his reputation.  I expand on these thoughts below.
<!-- PELICAN_END_SUMMARY -->

Naively, the odds of picking 63 games correctly are as follows.  If
the outcome of a single game was random, the odds of either team
winning (or of correctly picking the winner) would be 1 in 2, or 50%.
The odds of anyone picking the correct outcome of 2 games (team A vs
team B; and team C vs team D) are 1 in 4 (either A and C win; A and D
win; B and C win; or B and D win), or 25%.  A pattern quickly emerges:
the chance of picking $N$ games correctly goes like $1/2^N$.  This
generalizes even further: assuming odds $O$ of any particular choice
being correct (which is 50%, or 0.5, above), the likelihood of picking
$N$ games correct is $O^N$.  Naively assuming odds of 0.5 for each of
63 games in the tournament, the chances of getting them all correct is
$0.5^{63} = 1.1 \times 10^{-19}$, roughly 1-to-$9.2 \times
10^{18}$, or 9.2 billion billion.  This is also approximately the
number of grains of
[sand](http://www.npr.org/blogs/krulwich/2012/09/17/161096233/which-is-greater-the-number-of-sand-grains-on-earth-or-stars-in-the-sky)
on the entire planet Earth.  **Such is the power of exponential growth**,
that you can double something, double it again, and by doing this 63
times you have an entire planet's worth of material.  This process is not
unlike the growth in technology we currently find ourselves in the
midst of, where computing power is basically doubling every 18-24
[months](http://en.wikipedia.org/wiki/Moore%27s_law).  A quick
computation suggests that we are around 20 doublings into this process.

So in this ideal world, $\$9.2$ billion billion this should be the
monetary payout for correctly calling all 63 games of the NCAA
tournament, not a mere $\$1$ billion.  To think about this another
way, each dollar in that $\$1$ billion payout should itself be worth
$\$9.2$ billion.  The [vig](http://en.wikipedia.org/wiki/Vigorish) on
this bet would be enormous, perhaps unprecedented.

Now, the above assumes that the outcome of each game is basically a coin
flip.  But we know this to **not** be the case.  Teams are seeded
based on (among other considerations) their perceived strength, from 1
to 16, with #1 seeds expected to proceed farther into the tournament
than #16 seeds.  In addition, the first and sixteen seeds play each
other in the first round of the tournament, so this is not nearly a
coin flip.  In the WSJ article, Mr, Buffett is quoted as saying the
odds can't be calculated, and while this might be technically true
(no-one can predict the future), they can be estimated.  

Estimating The Odds
------------------------

To understand the true odds of calculating all 63 games of a NCAA
tournament correctly, one would ideally like many realizations of all
possible scenarios, to understand what fraction of the time the one
particular scenario of interest played out.  This is of course
impossible without the assistance of a Level-III
[multiverse](http://en.wikipedia.org/wiki/Multiverse), let alone that
there are 9.2 billion billion ways a single tournament in our Universe
could unfold.  So we take one step in a data-driven look at this
problem, which is to calculate the average odds $O$ that the favored
team beats the underdog.  This is somewhat of a "best case scenario",
in the sense that one could pick the next round of a bracket after
each previous one, picking only the higher seed of the two competitors
for any given game.

As a caveat, this is a very coarse parameterization of our ignorance of
just how the NCAA seeding rules create matchups that are able to be
predicted.  An additional complication is that the seeding rules yield
different teams in a given seed each year.  And each of these teams
will have in general different players and perhaps even coaches than
its previous entry.  This is the system complexity that Mr. Buffett
alludes to in the WSJ article.

To start this investigation, I obtained
[here](http://apps.washingtonpost.com/sports/apps/live-updating-mens-ncaa-basketball-bracket/search/?pri_school_id=&pri_conference=&pri_coach=&pri_seed_from=1&pri_seed_to=16&pri_power_conference=&pri_bid_type=&opp_school_id=&opp_conference=&opp_coach=&opp_seed_from=1&opp_seed_to=16&opp_power_conference=&opp_bid_type=&game_type=7&from=1985&to=2013&submit=)
a list of all the matchups in the NCAA basketball tournaments going
back to 1985.  From here it was a trivial computation to determine
what fraction of higher-seeded teams typically won.  Since there are 4
teams in each tournament with a given numerical seed, I have ignored all
competitions where equal-numbered seeds went against each other.

Results
------------------------------
I first split the data up into the 6 rounds: "First Round", "Second Round", "Sweet 16", "Elite Eight", "Final Four", "National Championship".
The fraction of top seeded teams that won in each round are listed below:

<div class="CSSTableGenerator" >
<table >
    <tr>
        <td>  Value                 </td>
        <td>                        </td>
        <td>  First Round           </td>
        <td>  Second Round          </td>
        <td>  Sweet 16              </td>
        <td>  Elite Eight           </td>
        <td>  Final Four            </td>
        <td>  National Championship </td>
    </tr>
    <tr>
        <td>  Total number of games </td>
        <td>                        </td>
        <td>  928  </td>
        <td>  464  </td>
        <td>  232 </td>
        <td>  116 </td>
        <td>  43  </td>
        <td>  23  </td>
    </tr>
    <tr>
        <td>  Number won by higher seed </td>
        <td>                     </td>
        <td>  693 </td>
        <td>  327 </td>
        <td>  166 </td>
        <td>  66  </td>
        <td>  28  </td>
        <td>  17  </td>
    </tr>
    <tr>
        <td>  Fraction won by higher seed </td>
        <td>                     </td>
        <td>  75% </td>
        <td>  70% </td>
        <td>  72% </td>
        <td>  57% </td>
        <td>  65% </td>
        <td>  74% </td>
    </tr>
</table>
</div>

As expected, the higher ranked seed performed exceptionally well in
the first round, winning approximately 75% of their games.  From the
second round on, this advantage is somewhat lessened, but there are
also fewer games in the sample.  Note, this analysis does *not* take
into account e.g. a particularly outstanding lower seed that left a
trail of correlated destruction in their wake (e.g. George Mason,
2006).  For the overall numbers, the higher seeds win 72% of their
games.

So how does this wrap into the bracket contest above?  We proceed by
looking at a "best case" scenario, where we can pick the results of
the "Second Round" after seeing the results of the "First Round", of
the "Sweet 16" after seeing the results of the "Second Round", etc.
First, use a likelihood of 0.75 for guessing all 32 "First Round"
games correctly, 0.70 for all 16 "Second Round" games, 0.72 for 8
"Sweet 16" games, 0.57 for 4 "Elite Eight" games, 0.65 for 2 "Final
Four" games, and 0.74 for the 1 Championship Game.  This yields a
1-in-1.4 billion chance that the higher ranked team wins every game in
the NCAA tournament, assuming you know the exact teams that go into
each and every game (in truth this is not how the tournament plays
out, as you have to fill out your bracket all at once, before any of
the 63 games are played).  However, the above numbers potentially
suffer from the process of "overfitting" where we have subdivided the
data too finely (e.g. per round) and end up multiplying together a
bunch of noisy, uncertain values.  If we instead use the overall
average odds of 0.72 for the higher seed to win, we find a much more
favorable scenario, a slightly worse than 1-in-1.1 billion chance of
the desired outcome.

No matter how you slice it, given the data on-hand, the likelihood of
picking a tournament correctly is exceedingly miniscule.  Only if you
were able to interactively pick your bracket after each round, and
only chose the favorites in each and every game, would you have a
1-in-a billion chance to win $\$1$ billion dollars.  Which is not too
shabby.  And the $\$11$ million premium being paid to insure the
money?  This is about 1% of the total prize; if the odds of winning
the prize were 1 in 100, this might seem like the appropriate level to
insure the kitty.  However, given the rules of the tournament, the
true likelihood of correctly guessing all 63 games is far less than
our best case scenario, which is still only a 1-in-1.1 billion chance.
Given this absolute best likelihood is worse than 1-in-1 billion, and
the value of the prize is (perhaps not coincidentally?) $\$1$ billion,
the true premium on insuring this bet should likely be less than
$\$1$.

Bottom line, this is great bet for Berkshire Hathaway to make, and
unnecessary insurance for the sponsor of the competition.  It is,
however, good PR for all involved.